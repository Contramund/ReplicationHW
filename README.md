# ReplicationHW

It just works )
![image](https://github.com/Contramund/ReplicationHW/assets/61385404/3967175e-5a59-4a88-9848-bd3d1674ecf3)


## Общая структура проекта: 

* `main.go` -- парсит аргументы командной строки, затем инициирует менеджер транзакций, клиента и сервер репликации
* `storage.go` -- реализация менеджера транзакций: транзакции принимает через канал (так как много писателей), читатели обращаются напрямую к менеджеру, RWMutex аннигилирует все проблемы с гонками
* `server.go` -- реализация сервера репликации: обслуживает все запросы по заданным роутам
* `transaction.go` -- просто структурка одной транзакции
* `index.html` -- формочка, чтобы удобно было потом отлаживать. По идее после копиляции должна автоматически встроиться в бинарник.

## Что нужно доделать:
* Верификация: нужно навесить фильтр на роут `/replace`, чтобы можно было менять только свои данные, иначе мало-ли чего запихают в патчи  
* Оптимизировать upstream канал: сейчас он спрашивает нет ли изменений раз в 3 секунды, но наверное нужно сделать поменьше
* Причесать логгирование: сейчас оно идет просто разрозненно в консоль. Нужно сделать отдельно в файл для всех запросов и отдельно в файл для обработки патчей
* Для наглядности заменить отсутствие времени при запросе `/ws` выдачей всего журнала. Сейчас считается, что спросивший ничего не знает и просто генерируется новый patch через "add" инструкцию для всех ников
* Тестирование: написать некоторые, по возможности рандомизированные, сценарии, чтобы проверить работает ли все, например:
  * На неполной топологии сети (теоретически должно)
  * При перезапуске одного из серверов с новым ником
  * При добавлении нового сервера с тем же ником (должен скачать себе данные этого ника и только потом что то менять уже над ними)
  * При некорректных патчах 
